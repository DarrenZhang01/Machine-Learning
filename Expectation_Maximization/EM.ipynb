{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference from \"https://people.duke.edu/~ccc14/sta-663/EMAlgorithm.html\"\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "np.random.seed(1000)\n",
    "\n",
    "from IPython.display import Image\n",
    "from numpy.core.umath_tests import matrix_multiply as mm\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import bernoulli, binom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function which calculates the log-likelihood of the data sample\n",
    "def neg_loglik(thetas, n, xs, zs):\n",
    "    likelihood = -np.sum([binom(n, thetas[z]).logpmf(x) for (x, z) in zip(xs, zs)])\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 8, 5, 7, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 10\n",
    "theta_A = 0.8\n",
    "theta_B = 0.3\n",
    "thetas = [theta_A, theta_B]\n",
    "\n",
    "coin_A = bernoulli(theta_A)\n",
    "coin_B = bernoulli(theta_B)\n",
    "\n",
    "xs = map(sum, [coin_A.rvs(m), coin_A.rvs(m), coin_B.rvs(m), coin_A.rvs(m), coin_B.rvs(m)])\n",
    "zs = [0, 0, 1, 0, 1]\n",
    "xs = np.array(list(xs))\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct MLE is: MLA 0.7333333333333333 MLB 0.35\n"
     ]
    }
   ],
   "source": [
    "ML_A = np.sum(xs[[0, 1, 3]]) / (3 * m)\n",
    "ML_B = np.sum(xs[[2, 4]]) / (2 * m)\n",
    "print(\"The correct MLE is: MLA {} MLB {}\".format(ML_A, ML_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 7.62865037343645\n",
       "     jac: array([ 0.00042597, -0.00062537])\n",
       " message: 'Converged (|f_n-f_(n-1)| ~= 0)'\n",
       "    nfev: 34\n",
       "     nit: 8\n",
       "  status: 1\n",
       " success: True\n",
       "       x: array([0.73333611, 0.34999288])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize using the neg_loglik function defined above\n",
    "minimize(neg_loglik, \n",
    "         [0.5, 0.5], \n",
    "         (m, xs, zs), \n",
    "         bounds = [(0, 1), (0, 1)], \n",
    "         method = \"TNC\",\n",
    "         options = {\"maxiter\": 100}) # This minimization uses truncated Newton Alogorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we don't know which coin it is? The actual Expectation Maximization Algorithm comes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration i: 0\n",
      "theta_A: [0.71301224 0.28698776], theta_B: [0.66 0.34], likelihood: -7.5591459965110435\n",
      "Iteration i: 1\n",
      "theta_A: [0.70259325 0.29740675], theta_B: [0.66 0.34], likelihood: -6.178236132586252\n",
      "Iteration i: 2\n",
      "theta_A: [0.6938765 0.3061235], theta_B: [0.66 0.34], likelihood: -6.182250042901686\n"
     ]
    }
   ],
   "source": [
    "xs = np.array([(5, 5), (9, 1), (8, 2), (4, 6), (7, 3)])\n",
    "thetas = np.array([[0.6, 0.4], [0.5, 0.5]])\n",
    "# The criterion for convergence\n",
    "tol = 0.01\n",
    "max_iter = 100\n",
    "likelihood = 0\n",
    "\n",
    "for i in range(max_iter):\n",
    "    ws_A = []\n",
    "    ws_B = []\n",
    "    vs_A = []\n",
    "    vs_B = []\n",
    "    likelihood_ = 0\n",
    "\n",
    "    # E-Step: Calculate probability distributions over possible completions\n",
    "    for x in xs:\n",
    "        E_A = np.sum([x * np.log(thetas[0])])\n",
    "        E_B = np.sum([x * np.log(thetas[1])])\n",
    "\n",
    "        # Use softmax function for calculating the weights\n",
    "        w_A = np.exp(E_A) / (np.exp(E_A) + np.exp(E_B))\n",
    "        w_B = np.exp(E_B) / (np.exp(E_B) + np.exp(E_B))\n",
    "\n",
    "        ws_A.append(w_A)\n",
    "        ws_B.append(w_B)\n",
    "\n",
    "        vs_A.append(np.dot(w_A, x))\n",
    "        vs_B.append(np.dot(w_B, x))\n",
    "\n",
    "        # Compute the updated likelihood\n",
    "        likelihood_ = w_A * E_A + w_B * E_B\n",
    "    \n",
    "    # M-Step: Update parameters given current distribution\n",
    "    thetas[0] = np.sum(vs_A, 0) / np.sum(vs_A)\n",
    "    thetas[1] = np.sum(vs_B, 0) / np.sum(vs_B)\n",
    "    \n",
    "    print(\"Iteration i: {}\".format(i))\n",
    "    print(\"theta_A: {}, theta_B: {}, likelihood: {}\".format(thetas[0], thetas[1], likelihood_))\n",
    "    \n",
    "    # If the update difference is less than a threshold, stop the loop\n",
    "    if (np.abs(likelihood - likelihood_) < tol):\n",
    "        break\n",
    "    likelihood = likelihood_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
